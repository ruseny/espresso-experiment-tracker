{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643ecd0e",
   "metadata": {},
   "source": [
    "# Building a RAG chat assistant\n",
    "\n",
    "This notebook develops the concept for a chat assistant that can give feedback on espresso-related questions, using the guidelines from [Espresso Aficionados website](https://espressoaf.com) as context.\n",
    "\n",
    "The idea is to refer to the properties of an espresso saved by the user, hence this is also integrated as an input to the prompt that will be sent to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2ee05",
   "metadata": {},
   "source": [
    "Models from Google AI Studio will be used: to export the Google API Key, which is stored in the .env file, load the environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d0028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feae827",
   "metadata": {},
   "source": [
    "## Indexing and storing context documents\n",
    "\n",
    "Unless or until the documents need to be updated, this can be done just once to save a persistent database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605c13c",
   "metadata": {},
   "source": [
    "Use the gemini embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc637cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model = \"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e186b",
   "metadata": {},
   "source": [
    "Create a Chroma vector database with persistent data written to the same directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53ad3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path = \"chroma_db\")\n",
    "chroma_vector_store = Chroma(\n",
    "    client = chroma_client,\n",
    "    collection_name = \"espresso_aficionados\",\n",
    "    embedding_function = gemini_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47181ad6",
   "metadata": {},
   "source": [
    "Get the relevant webpages, keeping only the main content, which is marked by the `<main>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9646e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "bs_main_content = bs4.SoupStrainer(\"main\")\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader_espresso_aficionados = WebBaseLoader(\n",
    "    web_paths = (\n",
    "        \"https://espressoaf.com/guides/beginner.html\", \n",
    "        \"https://espressoaf.com/guides/puckprep.html\", \n",
    "        \"https://espressoaf.com/guides/profiling.html\", \n",
    "        \"https://espressoaf.com/guides/water.html\", \n",
    "        \"https://espressoaf.com/guides/preferential-extraction.html\", \n",
    "        \"https://espressoaf.com/info/Glossary.html\", \n",
    "        \"https://espressoaf.com/info/flow_and_pressure.html\", \n",
    "        \"https://espressoaf.com/info/extraction_evenness_theory.html\"\n",
    "    ), \n",
    "    bs_kwargs = {\"parse_only\": bs_main_content}\n",
    ")\n",
    "\n",
    "docs_espresso_aficionados = loader_espresso_aficionados.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd90d7b",
   "metadata": {},
   "source": [
    "Split the page content into smaller chunks that can will comfortable fit into the LLM context window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd2f0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 100,\n",
    "    add_start_index = True\n",
    ")\n",
    "\n",
    "split_text_espresso_aficionados = text_splitter.split_documents(docs_espresso_aficionados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b1a80",
   "metadata": {},
   "source": [
    "Develop a procedure to add the split documents to the vector database in batches, so that Google API's rate limits for the embedding model are not exceeded (the limit is 100 requests per minute; to be on the safe side, we send 80 requests every 70 seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d235cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from time import sleep\n",
    "\n",
    "def add_documents_to_chroma_through_gemini(\n",
    "        documents = split_text_espresso_aficionados, \n",
    "        vector_store = chroma_vector_store\n",
    "    ):\n",
    "\n",
    "    # determine number of batches\n",
    "    if len(documents) == 0:\n",
    "        print(\"No documents found to be added to vector store.\")\n",
    "        return\n",
    "    elif len(documents) % 80 == 0:\n",
    "        n_batches = len(documents) // 80\n",
    "    else:\n",
    "        n_batches = len(documents) // 80 + 1\n",
    "\n",
    "    #iterate through batches\n",
    "    for i in range(n_batches):\n",
    "        batch = documents[i * 80 : (i + 1) * 80]\n",
    "        \n",
    "        # create unique ids for the batch\n",
    "        uuids_batch = [str(uuid4()) for _ in range(len(batch))]\n",
    "        \n",
    "        # add the batch to the vector store, if Google allows it\n",
    "        try:\n",
    "            vector_store.add_documents(\n",
    "                documents = batch,\n",
    "                ids = uuids_batch\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding chunk to vector store: {e}\")\n",
    "            raise\n",
    "        # if no exception was caught, wait for 70 seconds to avoid rate limiting\n",
    "        else:\n",
    "            print(f\"Added batch {i + 1} of {n_batches} to vector store.\")\n",
    "            sleep(70) \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390a828",
   "metadata": {},
   "source": [
    "Add the embeddings to the database with the function defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92338ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch 1 of 4 to vector store.\n",
      "Added batch 2 of 4 to vector store.\n",
      "Added batch 3 of 4 to vector store.\n",
      "Added batch 4 of 4 to vector store.\n"
     ]
    }
   ],
   "source": [
    "add_documents_to_chroma_through_gemini(\n",
    "    documents = split_text_espresso_aficionados,\n",
    "    vector_store = chroma_vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f301ca",
   "metadata": {},
   "source": [
    "## Retrieving context and generating response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac0ea9",
   "metadata": {},
   "source": [
    "Initialise the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "gemini_flash_llm = init_chat_model(\"gemini-2.5-flash\", model_provider = \"google_genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073541b",
   "metadata": {},
   "source": [
    "Prompt engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd092524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    You are an expert in espresso preparation and extraction. \n",
    "    You have access to the following guideline that contain relevant information about espresso preparation. \\n\n",
    "    Guidelines : {context}\n",
    "    The user will ask you a question about an espresso with the following properties: \\n\n",
    "    Espresso properties : {data}\n",
    "    Please answer the question at the end based on the provided context and data. \n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise. \\n\n",
    "    Question : {question}\n",
    "    Answer :\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdbdca3",
   "metadata": {},
   "source": [
    "Define espresso data to be used in the prompt (in the actual implementation, this would come from the database):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b06bae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class EspressoData(TypedDict):\n",
    "    roast_level: int\n",
    "    grind_size: float\n",
    "    dose_gr: float\n",
    "    extraction_time_s: int\n",
    "    yield_gr: float\n",
    "\n",
    "last_espresso = EspressoData(\n",
    "    roast_level = 4,\n",
    "    grind_size = 0.4,\n",
    "    dose_gr = 18.0,\n",
    "    extraction_time_s = 30,\n",
    "    yield_gr = 36.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2654e4ec",
   "metadata": {},
   "source": [
    "Define the logic of retrieval and generation. The app state consists of three inputs (context, data, question) and the answer. The context depends on the question, since the most relevant documents chunks will be retrieved based on similarity with the question, hence retrieval is a node in the flow. The data doesn't depend on the question, so it is simply parsed into a natural language description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c0f5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    context: List[Document]\n",
    "    data: EspressoData\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "def retrieve_context(state: State):\n",
    "    context_docs = chroma_vector_store.similarity_search(state[\"question\"], k = 2)\n",
    "    return {\"context\": context_docs}\n",
    "\n",
    "def describe_espresso_data(espresso_data: EspressoData) -> str:\n",
    "    return f\"\"\"\n",
    "        The espresso was made from beans with a roast level of {espresso_data[\"roast_level\"]} on a scale of 1 to 5, \n",
    "        ground to a size of {espresso_data[\"grind_size\"]} on a scale of 0 to 1,\n",
    "        with a dose of {espresso_data[\"dose_gr\"]} grams,\n",
    "        extracted for {espresso_data[\"extraction_time_s\"]} seconds,\n",
    "        yielding {espresso_data[\"yield_gr\"]} grams of espresso, \n",
    "        hence with an extraction ratio of {round(espresso_data[\"yield_gr\"] / espresso_data['dose_gr'], 2)}.\n",
    "        \"\"\"\n",
    "\n",
    "def generate_answer(state: State, espresso_data = last_espresso, llm = gemini_flash_llm, prompt = prompt) -> str:\n",
    "    docs_content = \"\\n\\n\".join([doc.page_content for doc in state[\"context\"]])\n",
    "    data_description = describe_espresso_data(espresso_data)\n",
    "    message = prompt.invoke({\n",
    "        \"context\" : docs_content,\n",
    "        \"data\" : data_description,\n",
    "        \"question\" : state[\"question\"]\n",
    "    })\n",
    "    response = llm.invoke(message)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454684c6",
   "metadata": {},
   "source": [
    "Bring together retrieval and generation in a langgraph object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c518682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve_context, generate_answer])\n",
    "graph_builder.add_edge(START, \"retrieve_context\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec82bcf",
   "metadata": {},
   "source": [
    "Test with a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a06acd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "    \"question\" : \"My espresso is too bitter, what can I do to improve it?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03b196b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      " [Document(id='eb80c520-66d0-48e9-bdbf-e4ae4802809b', metadata={'source': 'https://espressoaf.com/guides/beginner.html', 'start_index': 3}, page_content='How to Dial In Espresso: The Basics  A couple notes before we begin: 1) The dialling-in guide below is only one of the many ways an espresso shot can dialled. There are variations to this technique, and many different shot profiles that one can use. That said, if you’ve just started your espresso journey, I’ve found this particular step-by-step technique to be the easiest way to familiarise yourself with the basic principles of espresso extraction. 3) The 1:2 in 30 seconds “rule” is but a'), Document(id='5bba32df-9bac-4145-95d1-06a258314e73', metadata={'start_index': 5585, 'source': 'https://espressoaf.com/guides/beginner.html'}, page_content='grind size. As a general rule, lower temperatures will extract less while higher temperatures will extract more.   “Help! My shot tastes bad no matter what I do!”  If you find that the shot doesn’t taste good no matter what, it could be your beans, not you! Try cupping your beans to get an overall sense of flavour and an indication of what your shot could taste like. If your beans taste overly sour or bitter during your cupping, chances are you won’t be able to get a shot that doesn’t taste')] \n",
      "\n",
      "\n",
      "Answer:\n",
      " Your espresso being too bitter suggests over-extraction. To reduce this, try lowering your brewing temperature, as the context states lower temperatures extract less and higher temperatures extract more. You could also try making your grind slightly coarser to reduce extraction, or consider cupping your beans to ensure they aren't inherently bitter. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Context:\\n\", result[\"context\"], \"\\n\\n\")\n",
    "print(\"Answer:\\n\", result[\"answer\"], \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "espresso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
